What we have done so far:

in run_pipeline.sh:

Step 1. We took a gpx data and extracted all the data from it into a json file
Step 1b. The JSON object was split into "chunks", determined by the max number of data points we want to send at one time
        (Most likely no more than 100 at a time)
Step 2. We found the latitude and longitude bounds of the whole route (max/min) and created a bbox around it (with some error margin)
Step 3. We pulled from Overpass API the bounded map data, and download as a .osm file
Step 4. A docker instance is spun up that runs osmium converter (to convert osm to a pbf file to use later)
Step 5. Another docker instance is run that runs osrm-backend. This does some pre-processesing to the map to make it 
        bike-friendly (optional), and create some files to aid in map-matching
Step 6. After the pre-processing, the docker container is opened on port 5000 and detached, so the next python script can
        communicate with it. It runs with the configuration of using MLD algorithm, with a max-matching size of 500.
        In reality this will never hit that, since our get requests must be much shorter due to character restrictions
Step 7. We call batch_route_calc.py to perform  multi-threaded GET requests to the docker instance. This takes
        each JSON chunk file, and performs a match algorithm to create a route that closely matches the path taken.
        7a.) Within the batch calculation, the search radius is updated per point, depending on how many heading changes
        occur in a given area
Step 8. Each sub-route from the chunk files is ordered, and merged together to create the full route match
        This uses some algorithms to match separated routes



Backup if no success in matching a chunk... instead, take previous chunk's last coordinate and next chunk's first, and perform a route request on that instead. Route becomes that chunk (give it some way of showing it may not be correct)
        
