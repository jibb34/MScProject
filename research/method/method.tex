\documentclass[12pt,a4paper]{report}

\usepackage[a4paper, margin=1in]{geometry}
\geometry{margin=1in}
\usepackage{titlesec}
\usepackage{listings}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{dirtytalk}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{hyperref}
\usepackage{setspace}
\usepackage{lmodern}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}

\lstset{
  basicstyle=\ttfamily\small,
  keywordstyle=\bfseries\color{blue},
  commentstyle=\itshape\color{gray},
  stringstyle=\color{red},
  numbers=left,
  numberstyle=\tiny,
  frame=single,
  tabsize=2,
  captionpos=b,
  breaklines=true,
  showstringspaces=false,
  aboveskip=0.5em,        % less space above
  belowskip=0.5em,        % less space below
  lineskip=-1pt,          % tighter lines
  xleftmargin=0.5em,
  xrightmargin=0.5em
}
\onehalfspacing
\begin{document}

\tableofcontents
\newpage
\section{Methodology}

\subsection{Pre-processing}
Firstly we must obtain a GPX file. This is give from either a new or existing activity. It can be downloaded in many ways, either from common fitness websites (Strava, Garmin, MapMyRide, etc.), or directly
downloaded from a GPS device. Either way, there are some standards we can take advantage of. Each GPX file has the following structure:
\\ <insert Structure here>
\\
Once downloaded, they must be normalized and processed. This is done simply by converting to a standard JSON structure, where each object contains at least Latitude, Longitude, Elevation, and timing data.
After this, some processing is done to remove any erroneous values. Simply detecting 0s or NaN values in the data stream is sufficient, and the missing data is interpolated with the average of the surrounding values.
Next, the speed values are calculated along with heading, this is done mathematically using the Haversine formula, and added as an extension in the JSON file.

\subsection{Pulling Map Data}
A useful library for python called OSMnx exists, which allows easy and efficient pulling of API data from the OpenStreetMap database. This is what we will use to correct the GPS data, and to enrich the data for segmentation and categorization.
To do this, we use the \texttt{os.graph\_from\_bbox()} function to pull only nearby data (within the bounding box of the route). The approach is as follows:
\begin{enumerate}
	\item Find the largest and smallest coordinates in the route
	\item Generate a bounding box of the extreme coordinates
	\item Send the bounding box to a function that pulls the data from OSM
\end{enumerate}


%TODO: AI SECTION: REPLACE WITH REAL TEXT
\section{Map Matching with OSRM}

To accurately reconstruct the route a cyclist has taken from a GPS track, this project makes use of the \texttt{/match} service provided by the Open Source Routing Machine (OSRM). Unlike the \texttt{/route} endpoint, which simply computes a path between two or more points, \texttt{/match} performs map matching by snapping a noisy sequence of GPS points to the road network. This is particularly useful for real-world GPX tracks, which often contain small deviations due to GPS error.

\subsection{Request Format}

The OSRM \texttt{/match} service accepts both GET and POST requests. For long sequences (e.g., thousands of GPS points), a POST request with a JSON payload is required. The typical JSON body has the following structure:

\begin{lstlisting}[language=json]
{
  "coordinates": [[lon1, lat1], [lon2, lat2], ...],
  "timestamps": [timestamp1, timestamp2, ...],
  "radiuses": [5, 5, 5, ...],
  "geometries": "geojson",
  "steps": false
}
\end{lstlisting}

Here, \texttt{coordinates} is a list of longitude-latitude pairs, \texttt{timestamps} is optional and provides time information for each point (in Unix time), and \texttt{radiuses} specifies the positional uncertainty in meters.

\subsection{Response Structure}

The response contains a series of \texttt{matchings}, each of which includes a snapped geometry and additional metadata. Key fields include:

\begin{itemize}
	\item \texttt{matchings[*].geometry}: A GeoJSON LineString of the matched route.
	\item \texttt{tracepoints}: An array of points indicating how each GPS observation was matched.
	\item \texttt{matchings[*].legs[*].annotation.nodes}: A list of OSM node IDs representing the matched path.
\end{itemize}

Note that OSRM does not return OpenStreetMap \texttt{way\_id}s directly. However, with additional preprocessing or Overpass API queries, these node sequences can be translated back into way identifiers if needed.

\subsection{Python Integration}

The following Python function sends a GPS trace to the OSRM \texttt{/match} service:

\begin{lstlisting}[language=Python]
import requests

def osrm_match(points, timestamps=None, host="http://localhost:5000", profile="bicycle"):
    coords = [[lon, lat] for lat, lon in points]
    url = f"{host}/match/v1/{profile}"
    body = {
        "coordinates": coords,
        "geometries": "geojson",
        "overview": "full",
        "steps": False
    }
    if timestamps:
        body["timestamps"] = timestamps
    response = requests.post(url, json=body)
    return response.json()
\end{lstlisting}

This pipeline enables full reconstruction of a matched route from raw GPS input, suitable for further analysis such as segment extraction, elevation profiling, or training suitability scoring.



\subsection{Segmentation}
\subsection{Categorization}
\subsection{Segment Analysis}
\subsubsection{Wavelet Transform}
Sharifzadeh et al. (2005) propose the use of the Wavelet Transform Function, and more specifically Wavelet Footprints,
to address the problem of change detection in time series data.
\begin{itemize}
	\item \textbf{Wavelet Decomposition of Trajectory}: \\ Given a signal $x[n]$ of length $N$, it can be expressed as a wavelet series:
	      \[
		      x[n] =
		      \sum_{j=0}^{J-1} \sum_{k} d_{j,k} \, \psi_{j,k}[n]
		      + \sum_{k} c_{J,k} \, \phi_{J,k}[n]
	      \]
	      where:
	      \begin{itemize}
		      \item $\psi_{j,k}[n]$ are the wavelet basis functions at scale $j$ and position $k$.
		      \item $\phi_{J,k}[n]$ are the scaling functions at the coarsest level $J$.
		      \item $d_{j,k}$ are detail coefficients at scale $j$ and position $k$.
		      \item $c_{J,k}$ are approximation coefficients.
	      \end{itemize}
	\item \textbf{Footprint Definition}\\
	      The \textbf{wavelet footprint} of $x$ is defined as the set of significant coefficients:
	      \[
		      F(x) = \{ (j,k) \;|\; |d_{j,k}| > T_j \}
	      \]
	      where $T_j$ is a scale-dependent threshold, which can be tuned to correspond to physiological thresholds of training intensity zones. This is useful since
	      at different scales, the detail coefficients are not comparable, so the threshold must be defined explicitly.

	\item \textbf{Footprint Calculation}\\
	      To calculate the Wavelet Footprint, first the detail coefficient, $d_{j,k}$ is extracted. Since not all detail coefficients are meaningful, a scale-dependent threshold $T_j$
	      is defined to filter out noise. This is calculated with the standard deviation of the specific scale:
	      \[
		      T_j = \gamma_j \dot \sigma_j
	      \]

	      Where:
	      \begin{itemize}
		      \item $\sigma_j$ is the standard deviation of $d_{j,k}$ at scale $j$.
		      \item $\gamma_j$ is the tuning parameter.
	      \end{itemize}
	      This value can then be used to define the Footprint Definition described earlier.

	\item \textbf{Compact Representation}\\
	      To reduce dimensionality, only the $k$ largest coefficients can be retained:
	      \[
		      F_k(x) = \text{top-}k \text{ elements of } F(x)
	      \]
	      This is useful to optimize data set size, while not reducing the accuracy of the data significantly.
	\item \textbf{Matching and Distance}\\
	      The overall difference in two signals $x$ and $y$ can be approximated by taking the difference of their footprints:
	      \[
		      D(x,y) \approx \| F(x) - F(y) \|
	      \]

\end{itemize}
\subsubsection{Normalized Power}
Applying Normalized Power is a useful metric for determining variability in power data. The longer the signal, the more useful NP would be, so
we can apply a weighting inversely proportional to the average duration of the segment.
However, as the time gets shorter, we can mitigate the accuracy loss by tapering off the 30 second rolling average towards the end of the segment.
\\
Instead of a 30 second rolling average, we will take the lesser of either 30 seconds, or the remaining duration of the segment, so we don't end up
with a lot of segments with skewed NP due to the rolling window running off the edge of the data set.\\

We will define a variable $k$ that represents this:
\[
	k[t] = \begin{cases}
		(T-t), & (T-t) < 30   \\
		30,    & (T-t) \ge 30 \\
	\end{cases}
\]
Now we can define the rolling average, ${P}_{r}$ at time $t$ to be:
\[
	\overline{P}_{r}[t] = \frac{1}{k[t]} \sum_{j=0}^{k[t]-1} P[j]
\]
The final definition of NP is:
\[
	\mathrm{NP} = \bigg( \frac{1}{M} \sum_{i=1}^{M}\big(\overline{P}_{r}[i]\big)^{4}\bigg)^{1/4}
\]


\subsection{Training Suitability Score Evaluation}
\end{document}
